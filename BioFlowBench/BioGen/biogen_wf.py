import os
import json
import re
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser


from tools.genomics_tools import simulate_dna_reads_paired, get_seed_file_path
from tools.transcriptomics_tools import simulate_rna_seq_reads_rsem, simulate_rna_seq_reads_art
from tools.variomics_tools import align_reads_bwa, call_variants_bcftools, simulate_variants_msprime
from tools.proteomics_tools import simulate_ms_spectra_pyopenms
from tools.metagenomics_tools import simulate_metagenome_insilicoseq, create_genome_list_file
from tools.metabolomics_tools import simulate_metabolomics_peak_list


class TaskStep(BaseModel):
    step_id: int = Field(description="The unique, zero-indexed identifier for this step.")
    tool: str = Field(description="The name of the tool to be executed.")
    input: Dict[str, Any] = Field(description="Arguments for the tool. May contain references like $output_of_step_X.")
    output: Optional[Dict[str, str]] = Field(default=None, description="Output paths filled by executor.")

class WorkflowPlan(BaseModel):
    goal: str = Field(description="Description of the overall scientific goal of the workflow.")
    synthesis_steps: List[TaskStep] = Field(description="Steps to generate input files for the FIRST tool in the workflow sequence.")
    workflow_execution_logic: str = Field(description="A textual description or template of how the Target Workflow (Tools 1 to N-1) should be executed sequentially, creating intermediate files.")
    validation_run_command: str = Field(description="The command template for the LAST tool (Validation Tool) which takes the final output of the workflow as input.")

class BenchmarkOutput(BaseModel):
    user_query: str = Field(description="The natural language request asking the agent to run the sequential workflow (excluding the validation tool).")
    ground_truth_script: str = Field(description="A shell script or sequence of commands executing the Target Workflow (Tools 1 to N-1) sequentially.")
    validation_command: str = Field(description="The executable command for the Validation Tool (Tool N) to verify the result.")


class BioDataForgeAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            base_url="",
            api_key="",
            model="",
            temperature=0.1
        )
        
        self.seed_data = self._get_seed_data() 
        self.tools_info = self._get_tools_info()
        
        self.tool_functions = {
            "get_seed_file_path": get_seed_file_path,
            "simulate_dna_reads_paired": simulate_dna_reads_paired,
            "simulate_rna_seq_reads_rsem": simulate_rna_seq_reads_rsem,
            "simulate_rna_seq_reads_art": simulate_rna_seq_reads_art,
            "align_reads_bwa": align_reads_bwa,
            "call_variants_bcftools": call_variants_bcftools,
            "simulate_variants_msprime": simulate_variants_msprime,
            "simulate_ms_spectra_pyopenms": simulate_ms_spectra_pyopenms,
            "simulate_metagenome_insilicoseq": simulate_metagenome_insilicoseq,
            "create_genome_list_file": create_genome_list_file,
            "simulate_metabolomics_peak_list": simulate_metabolomics_peak_list,
        }

        self.parser = PydanticOutputParser(pydantic_object=WorkflowPlan)
        self.benchmark_parser = PydanticOutputParser(pydantic_object=BenchmarkOutput)
        
        self.prompt = self._setup_prompt_template()
        self.benchmark_prompt = self._setup_benchmark_prompt()

    def _get_seed_data(self):
        return [
                {
                  "filepath": "./bio_seeds/Genomics/chrM.fa",
                  "domain": "Genomics",
                  "description": "Human mitochondrial reference genome sequence. In FASTA format, sourced from the UCSC Genome Browser (hg38 assembly). It serves as the fundamental basis for all sequence alignment and coordinate-based analyses, such as variant calling and RNA-Seq alignment."
                },
                {
                  "filepath": "./bio_seeds/Genomics/chrM.fa.fai",
                  "domain": "Genomics",
                  "description": "FASTA index file. Generated by `samtools faidx` to enable fast, random access to the `chrM.fa` file. This file is a mandatory requirement for many bioinformatics tools (e.g., GATK, bcftools) when processing a reference genome."
                },
                {
                  "filepath": "./bio_seeds/Transcriptomics/chrM.gtf",
                  "domain": "Transcriptomics",
                  "description": "Human mitochondrial gene annotation file. In GTF format, with `chrM`-related entries extracted from the complete Ensembl annotation file (release 104). It defines the locations and structures of mitochondrial genes, transcripts, and exons, and is critical for applications like RNA-Seq quantification."
                },
                {
                  "filepath": "./bio_seeds/Variomics/header.vcf",
                  "domain": "Variomics",
                  "description": "VCF header template. A manually created header snippet compliant with the VCFv4.2 specification. Its most critical role is to define the contig (`contig=<ID=chrM,length=16569>`), ensuring that subsequently generated VCF files are compatible with the `chrM.fa` reference genome."
                },
                {
                  "filepath": "./bio_seeds/Proteomics/uniprot_sprot.fasta",
                  "domain": "Proteomics",
                  "description": "UniProt/Swiss-Prot protein database. A compressed FASTA file containing high-quality, manually reviewed, and annotated protein sequences. It is the standard reference library for peptide matching in mass spectrometry-based protein identification."
                },
                {
                  "filepath": "./bio_seeds/Metagenomics/16s_seeds.fa",
                  "domain": "Metagenomics",
                  "description": "Representative 16S rRNA gene sequences. A FASTA file containing 16S rRNA gene sequences from model organisms (e.g., E. coli, B. subtilis), downloaded from NCBI. Used for simulating and testing species classification workflows based on 16S amplicon sequencing."
                },
                {
                  "filepath": "./bio_seeds/Metabolomics/metabolites.tsv",
                  "domain": "Metabolomics",
                  "description": "Sample metabolite list. A simple Tab-Separated Values (TSV) file listing several common metabolites with their IDs, names, and chemical formulas. It simulates a structured data table used for annotation or querying in metabolomics analysis."
                }
        ]

    def _get_tools_info(self):
        return [
            {
              "toolname": "get_seed_file_path",
              "domain": "all",
              "input": "filepath: str",
              "output": "seed filepath: str",
              "description": "Returns the full path to a file in the seed repository.Use this to get paths for reference genomes, annotations, etc."},
            {
            "toolname": "simulate_dna_reads_paired",
            "domain": "all",
            "input": "reference_fasta: str, num_reads: int = 1000, mutation_rate: float = 0.001",
            "output": "DNA reads generated (path_to_fastq_file1: str, path_to_fastq_file2: str)",
            "description": "Simulates paired-end DNA reads from a reference FASTA file using wgsim. Returns the paths to the two generated FASTQ files."
            },
            {
              "toolname": "simulate_metabolomics_peak_list",
              "domain": "Metabolomics",
              "input": "num_compounds: int = 100, num_samples: int = 20, noise_level: float = 0.1",
              "output": "Metabolomics peak list CSV generated filepath: str",
              "description": "Simulates a metabolomics peak list using Numpy and Pandas. Generates a CSV file containing m/z, retention time, and intensity for each compound across multiple samples. Returns the path to the generated CSV file."
            },
            {
              "toolname": "simulate_metagenome_insilicoseq",
              "domain": "Metagenomics",
              "input": "genome_list_file: str, total_reads: int = 1000000, abundance_model: str = 'lognormal'",
              "output": "Metagenomic reads generated by InSilicoSeq: read1_filepath: str, read2_filepath: str",
              "description": "Simulates paired-end metagenomic reads using InSilicoSeq (iss). Requires a file listing the paths to input genome FASTA files. Abundance is randomly generated based on a specified model. Returns the paths to the two generated FASTQ files."
            },
            {
              "toolname": "create_genome_list_file",
              "domain": "Metagenomics",
              "input": "genome_paths: list of str",
              "output": "Created genome list file : file path (genome_list.txt)",
              "description": "Creates a text file where each line is the full path to a genome file. It takes a list of genome filenames (from the seed repository) and returns the path to the newly created file. This is a helper function for tools like InSilicoSeq."
            },
            {
              "toolname": "simulate_ms_spectra_pyopenms",
              "domain": "Proteomics",
              "input": "protein_fasta: str, enzyme: str = 'Trypsin'",
              "output": "mzml_filepath: str",
              "description": "Simulates a mass spectrometry (LC-MS/MS) experiment from a protein FASTA file using pyOpenMS. The process includes in-silico protein digestion, peptide generation, and mass spectra simulation. Returns the file path of the resulting mzML file."
            },
            {
            "toolname": "simulate_rna_seq_reads_art",
            "domain": "all",
            "input": "reference_transcriptome_fasta: str, num_reads_per_transcript: int = 50, read_length: int = 100",
            "output": "A tuple of two strings, representing the file paths for the generated paired-end FASTQ files (read 1 and read 2): (str, str)",
            "description": "Simulates paired-end RNA-Seq reads from a reference transcriptome FASTA file using the 'art_illumina' tool. This simple model generates a fixed number of reads for each transcript."
            },
            {
            "toolname": "simulate_rna_seq_reads_rsem",
            "domain": "Transcriptomics",
            "input": "reference_transcriptome_fasta: str, total_reads: int = 1000000",
            "output": "A tuple of two strings, representing the file paths for the generated paired-end FASTQ files (read 1 and read 2): (str, str)",
            "description": "Simulates paired-end RNA-Seq reads using RSEM's 'rsem-simulate-reads'. RSEM can simulate more realistic expression profiles based on a model file; this tool uses a default uniform expression profile. It automatically prepares the necessary RSEM reference index before simulation."
            },
            {
            "toolname": "align_reads_bwa",
            "domain": "Variomics",
            "input": "reference_fasta: str, reads_r1_fastq: str, reads_r2_fastq: str",
            "output": "sorted_bam_path: str",
            "description": "Aligns paired-end reads to a reference genome using BWA-MEM. Returns the path to the sorted BAM file."
            },
            {
            "toolname": "call_variants_bcftools",
            "domain": "Variomics",
            "input": "sorted_bam: str, reference_fasta: str",
            "output": "vcf_path: str",
            "description": "Performs variant calling from a sorted BAM file using bcftools. Returns the path to the generated VCF file."
            },
            {
            "toolname": "simulate_variants_msprime",
            "domain": "Variomics",
            "input": "sample_size: int = 10, length: int = 10000",
            "output": "vcf_path: str",
            "description": "Directly simulates variants using msprime based on a population genetics model. Returns the path to the generated VCF file."
            }
        ]

    def _setup_prompt_template(self):
        SYSTEM_PROMPT = '''
        You are BioDataPlanner. You design workflow test scenarios.
        
        **Your Input:** A list of tools representing a sequential workflow: [Tool_1, Tool_2, ..., Tool_N].
        *   **Target Workflow:** Tools [Tool_1 ... Tool_(N-1)]. The agent being tested must run these.
        *   **Validation Tool:** Tool_N. This is used ONLY to verify the output of Tool_(N-1).

        **Your Task:** Generate a JSON `WorkflowPlan`:
        1.  **Data Synthesis:** Design steps to create the specific input required by **Tool_1** (the first tool).
            *   Start from "Seed Files".
            *   Use ONLY "Available Synthesis Tools".
        2.  **Workflow Execution Logic:** Describe how to run Tool_1 -> ... -> Tool_(N-1). 
            *   Define explicit filenames for intermediate outputs (e.g., `step1_out.txt`, `final_result.bed`).
        3.  **Validation Command:** Describe the command for **Tool_N** (Validation Tool).
            *   Its input MUST be the output filename of Tool_(N-1).
        4. You only need to provide the specific input required by **Tool_1**. Do not provide any other input.
        **Critical Rules:**
                  1.  **Synthesis Steps:** ONLY use tools from "Available Synthesis Tools".
                  2.  **Dependency Syntax:** In `synthesis_steps`, refer to previous outputs as `$output_of_step_X` or `$output_of_step_X[i]`.
        **Available Resources:**
        *   Seeds: {seed_data}
        *   Synthesis Tools: {tools}

        {format_instructions}
        '''
        
        human_template_str = """
        **Requested Workflow Sequence:**
        {workflow_description}

        Please design the Synthesis steps for the FIRST tool's input, and the execution/validation logic for the rest.
        """
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", human_template_str),
        ])
        return prompt.partial(
            tools=json.dumps(self.tools_info, indent=2),
            seed_data=json.dumps(self.seed_data, indent=2),
            format_instructions=self.parser.get_format_instructions()
        )

    def _setup_benchmark_prompt(self):
        SYSTEM_PROMPT = """
        You are **BioBenchmarkGenerator**.
        
        **Context:**
        We have a linear workflow: [Tool_1, ..., Tool_(N-1)] -> [Tool_N (Validation)].
        We have executed "Data Synthesis" to generate the input file for Tool_1.

        **Your Task:** Create the final Benchmark JSON.
        1.  **user_query:** A natural language request asking to run the workflow (Tool_1 to Tool_(N-1)).
            *   Do NOT verify. Just ask to process the data.
            *   Include the *actual* synthesized input file path for Tool_1.
        2.  **ground_truth_script:** The exact commands to run Tool_1, then Tool_2, etc., up to Tool_(N-1).
            *   Use valid file paths. Invent filenames for intermediate and final outputs.
        3.  **validation_command:** The command to run Tool_N on the final output of the workflow.
        4. Do not give the tool to use, just give the question to ask the user. Do not tell the user the detail steps in the question, just show the final output of the workflow required in the question.

        {format_instructions}
        """
        
        human_template_str = """
        **Workflow Info:**
        {workflow_info_str}

        **Synthesized Data (Input for Tool_1):**
        {executed_steps_json}

        **Action:** Generate `BenchmarkOutput` JSON.
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", SYSTEM_PROMPT),
            ("human", human_template_str),
        ])
        return prompt.partial(format_instructions=self.benchmark_parser.get_format_instructions())

    def create_plan(self, tools_sequence: List[Dict]) -> WorkflowPlan:
        """
        tools_sequence: A list of dicts, each describing a tool in order.
        """
        print("Generating a workflow plan for the tool sequence...")
        
        workflow_desc = ""
        for i, tool in enumerate(tools_sequence):
            role = "VALIDATION TOOL" if i == len(tools_sequence) - 1 else f"Step {i+1}"
            workflow_desc += f"\n[{role}]: {tool['name']} (v{tool.get('version', 'N/A')})\n"
            workflow_desc += f"  - Description: {tool['desc']}\n"
            workflow_desc += f"  - Command: {tool['cmd_template']}\n"
            workflow_desc += f"  - Input Need: {tool.get('input_type', 'unknown')}\n"

        chain = self.prompt | self.llm | self.parser
        plan = chain.invoke({"workflow_description": workflow_desc})
        print("Plan generated successfully!")
        return plan

    def _resolve_input_value(self, value, step_outputs):
        if isinstance(value, str):
            match_index = re.match(r"^\$output_of_step_(\d+)\[(\d+)\]$", value)
            if match_index:
                step, idx = map(int, match_index.groups())
                return step_outputs[step][idx]
            match = re.match(r"^\$output_of_step_(\d+)$", value)
            if match:
                return step_outputs[int(match.group(1))]
            return value
        elif isinstance(value, list):
            return [self._resolve_input_value(item, step_outputs) for item in value]
        elif isinstance(value, dict):
            return {k: self._resolve_input_value(v, step_outputs) for k, v in value.items()}
        return value

    def execute_plan(self, plan: WorkflowPlan) -> WorkflowPlan:
        print(f"--- Executing Synthesis Steps ---")
        step_outputs = {}
        for step in plan.synthesis_steps:
            print(f"  [Step {step.step_id}] Tool: {step.tool}")
            resolved_inputs = {k: self._resolve_input_value(v, step_outputs) for k, v in step.input.items()}
            
            tool_obj = self.tool_functions.get(step.tool)
            if not tool_obj:
                print(f"Warning: Tool {step.tool} function not found/mocked.")
                step.output = {"result": "/mock/path/result.file"} 
                step_outputs[step.step_id] = "/mock/path/result.file"
                continue
                
            try:
                output = tool_obj.func(**resolved_inputs) 
                step_outputs[step.step_id] = output
                step.output = {"result": output}
                print(f"Output: {output}")
            except Exception as e:
                print(f"Error: {e}")
                raise
        return plan

    def generate_final_benchmark(self, executed_plan: WorkflowPlan, tools_sequence: List[Dict]) -> BenchmarkOutput:
        print("\nGenerating Final Benchmark Case...")
        
        executed_steps_summary = []
        for step in executed_plan.synthesis_steps:
            executed_steps_summary.append({
                "step_id": step.step_id,
                "tool": step.tool,
                "actual_output": step.output
            })

        workflow_info_str = json.dumps(tools_sequence, indent=2)

        chain = self.benchmark_prompt | self.llm | self.benchmark_parser
        result = chain.invoke({
            "workflow_info_str": workflow_info_str,
            "executed_steps_json": json.dumps(executed_steps_summary, indent=2)
        })
        return result

if __name__ == '__main__':
    agent = BioDataForgeAgent()
    
    workflow_tools =[]

    try:    
        generated_plan = agent.create_plan(tools_sequence=workflow_tools) 
        print("\n--- Generated Workflow Plan ---")
        print(json.dumps(generated_plan.model_dump(), indent=2))    
        executed_plan = agent.execute_plan(generated_plan)
        final_benchmark = agent.generate_final_benchmark(executed_plan, workflow_tools)

    except Exception as e:
        print(f"\nAn error occurred: {e}")
        import traceback
        traceback.print_exc()